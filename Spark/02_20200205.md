# Spark: Day 2

## Spark 설치 - databricks
- 커뮤니티 버전을 사용할 시에는 로그인 화면에서 커뮤니티 버전으로 들어가야 한다.

## Spark 설치 - docker
### Docker 이미지 실행하기
1. virtual box에서 모두 삭제, 삭제 centos, hdp
2. docker quickstart terminal 오픈 후 아래 내용 실행
    - docker downlaod

    ```
    docker pull docker.io/rheor108/spark_the_definitive_guide_practice
    ```
    - docker 실행
    ```
    docker run -p 8080:8080 -p 4040:4040 rheor108/spark_the_definitive_guide_practice
    ```
    - docker가 실행이 되면 아래 URL로 Zeppeline이 실행되는 것을 확인하면 된다.
        - http://localhost:8080
    - 만일 접속이 안된다면 Kitematic (Alpha)를 통해 접속한다.
        - Containers에 spark_the_definitive_guide_practice만 떠있게 한 후 WEP PREVIEW 첫번째 버튼

## 스파크 구성
```
Spark SQL | Spark Streaming | MLlib(machine learning) | GraphX
                        Apache Spark
```


- Apache Spark : core - scala

## 스파크 동작방식
```
Driver Program <-> cluster manager <-> Worker Node 1  
  SparkSession                             Executor  
                                           Task  
                                           Task  
                                   <-> Worker Node 2
                                           Executor  
                                           Task  
                                           Task  
```

## 용어 정리
- `driver program`: 프로그래밍 언어의 main() 함수와 같은 역할
- `SparkSession`: 어떻게 스파크 클러스터에 접근할 수 있는지를 알려주는 Object or session
- `cluster manager`: 클러스터에 필요한 자원들을 찾아줌
- `worker node`: 실제 작업을 수행하는 노드
- `job`: 사용자 입장에서의 작업의 단위(task의 조합)
- `task`: executor에 할당되는 작업의 단위
- `executor`: task를 수행하는 프로세스

## RDD(Resilient Distributed Datasets)
- immutable collection
- parallel processing
```
data -> worker node 1 data
     -> worker node 2 data
     -> worker node 3 data
```
- fault tolerant
- 데이터형이 정해져 있음
- type safe

> RDD의 장점
- type safety -> compile time check
- low level(저수준 API) -> fine tuning(쾌적하게 시스템 튜닝)
    - if 본인이 전문가일 경우, 구루일 경우

> RDD의 단점
- (다른 언어로 접근했을 시) 느림
    - 그래서 RDD 사용할 때는 Scala로 하는 게 좋음
    - RDD를 보완해서 나오는 게 DataFrame 등등

## transformation
- DAG에는 나타남
- 데이터를 내가 원하는 모양으로 조각하는 것
- transformation의 결과는 RDD
    - val learKingText = sc.textFile("/FileStore/tables/pg1532.txt") -> `in databricks`
    - val learKingText = spark.sparkContext.textFile("/FileStore/tables/pg1532.txt") -> `in zeppelin`
    - 이렇게만 해놓은 상태로는 job에는 아무것도 없음
    - 실제 실행이 안된 것이고 실행되려면 action을 취해야 함
    - lazy 된다.

## action
- job이 시작이 된다.
    - ex. learnKingText.count()
- 조작한 데이터를 바탕으로 결과를 얻음
- Spark UI : http://localhost:4040 -> job
- node 1 (머신 1)인 경우 driver program == job
    - 드라이버 프로그램, job이 하나씩밖에 안생긴다.

## DataFrame
- SQL에 집중해서 데이터를 처리할 수 있다.
- spark에서도 optimize
- un type safe(타입이 지정되지 않음) 하기 때문에 데이터를 처리할 때는 마음대로 변형이 가능한데, 실제로 결과를 도출해서 저장하거나 할 경우 type을 지정해줘야 해서 번거로워질 수 있다.
- 데이터를 변형할 때는 좋지만, 마지막에 타입을 지정하는 것 때문에 데이터 조작할 때 얻었던 퍼포먼스를 잃어버릴 수 있다.

> 대용량데이터 필요할 때  
https://dumps.wikimedia.org/other/pagecounts-raw/2016/2016-01/pagecounts-20160101-000000.gz

## Plan Optimizationo & Execution
#### 속도 비교
![image](https://user-images.githubusercontent.com/58713684/73898747-aad80200-48cd-11ea-9880-934db275a962.png)
#### spark optimize 처리 파이프라인
![image](https://user-images.githubusercontent.com/58713684/73896968-e1ab1980-48c7-11ea-9845-8bd2faafa30f.png)
#### 실제로 spark가 처리하는 방법 예시
![image](https://user-images.githubusercontent.com/58713684/73896998-f687ad00-48c7-11ea-901d-8665b80533d6.png)
- Logical Plan은 join을 먼저 하기 때문에 cost 문제
- Spark 내에서 자동으로 Physical Plan 처리
