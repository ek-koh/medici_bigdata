# Spark: Day 2

## Spark 설치 - databricks
- 커뮤니티 버전을 사용할 시에는 로그인 화면에서 커뮤니티 버전으로 들어가야 한다.

## Spark 설치 - docker
### Docker 이미지 실행하기
1. virtual box에서 모두 삭제, 삭제 centos, hdp
2. docker quickstart terminal 오픈 후 아래 내용 실행
    - docker downlaod

    ```
    docker pull docker.io/rheor108/spark_the_definitive_guide_practice
    ```
    - docker 실행
    ```
    docker run -p 8080:8080 -p 4040:4040 rheor108/spark_the_definitive_guide_practice
    ```
    - docker가 실행이 되면 아래 URL로 Zeppeline이 실행되는 것을 확인하면 된다.
        - http://localhost:8080
    - 만일 접속이 안된다면 Kitematic (Alpha)를 통해 접속한다.
        - Containers에 spark_the_definitive_guide_practice만 떠있게 한 후 WEP PREVIEW 첫번째 버튼

## 스파크 구성
| Spark SQL | Spark Streaming | MLlib(machine learning) | GraphX |
|:---:|:---:|:---:|:---:|
### <center>Apache Spark</center>


- Apache Spark : core - scala

## 스파크 동작방식
```
Driver Program <-> cluster manager <-> Worker Node 1  
  SparkSession                             Executor  
                                           Task  
                                           Task  
                                   <-> Worker Node 2
                                           Executor  
                                           Task  
                                           Task  
```

## 용어 정리
- `driver program`: 프로그래밍 언어의 main() 함수와 같은 역할
- `SparkSession`: 어떻게 스파크 클러스터에 접근할 수 있는지를 알려주는 Object or session
- `cluster manager`: 클러스터에 필요한 자원들을 찾아줌
- `worker node`: 실제 작업을 수행하는 노드
- `job`: 사용자 입장에서의 작업의 단위(task의 조합)
- `task`: executor에 할당되는 작업의 단위
- `executor`: task를 수행하는 프로세스