# Spark: Day 3
## 실습 1
### 2016년 1월 요청이 제일 많은 단어 추출하기
- databricks에서 실습: 'sample03'  
  
## 실습 2
### 파티션
- 파티션 셔플을 하는데 기본적으로 200개의 셔플 파티션을 한다.
- 너무 많이 하지 않게 5정도로 값을 지정해준다.
```scala
spark.conf.set("spark.sql.shuffle.partitions", "5")
```

```
○ 스파크는 모든 익스큐터가 병렬로 작업을 수행할 수 있도록 파티션이라 불리는 청크 단위로 데이터를 분할한다.  

○ 파티션은 클러스터의 물리적 머신에 존재하는 로우의 집합을 의미한다.  

○ DataFrame의 파티션은 실행중에 데이터가 컴퓨터 클러스터에서 물리적으로 분산되는 방식을 나타낸다.  

○ 파티션이 하나라면, 스파크에 수천 개의 익스큐터가 있더라도 병렬성은 1이 된다.  

○ 또한 수백개의 파티션이 있더라도 익스큐터가 하나밖에 없다면 병렬성은 1이 된다. 

○ DataFrame을 사용하면 파티션을 수동 혹은 개별적으로 처리할 필요가 없다.   

○ 물리적 파티션에 데이터 변환용 함수를 지정하면 스파크가 실제 처리 방법을 결정한다.  

○ RDD 인터페이스를 이용하는 저수준 API역시 제공된다.  
```

### DataFrame/SQL
- createOrReplaceTempView를 사용해서 DataFrame을 테이블이나 뷰로 만들어보자.
```scala
flightData2015.createOrReplaceTempView("flight_data_2015")
```
- SQL로 데이터 처리하기
```scala
// in Scala
val sqlWay = spark.sql("""
SELECT DEST_COUNTRY_NAME, count(1)
FROM flight_data_2015
GROUP BY DEST_COUNTRY_NAME
""")

val dataFrameWay = (flightData2015
  .groupBy('DEST_COUNTRY_NAME)
  .count())

sqlWay.explain
dataFrameWay.explain
```
```scala
spark.sql("SELECT max(count) from flight_data_2015").take(1)
```
```
// in Scala
import org.apache.spark.sql.functions.max

flightData2015.select(max("count")).take(1)
```
```scala
// in Scala
val maxSql = spark.sql("""
SELECT DEST_COUNTRY_NAME, sum(count) as destination_total
FROM flight_data_2015
GROUP BY DEST_COUNTRY_NAME
ORDER BY sum(count) DESC
LIMIT 5
""")

maxSql.show()
```
```
+-----------------+-----------------+
|DEST_COUNTRY_NAME|destination_total|
+-----------------+-----------------+
|    United States|           411352|
|           Canada|             8399|
|           Mexico|             7140|
|   United Kingdom|             2025|
|            Japan|             1548|
+-----------------+-----------------+
```
- 의미는 같은데, 구현과 정렬 방식이 다른 DataFrame 구문
    - 실행계획은 SQL에서 한 것과 같음
    ```scala
    // in Scala
    import org.apache.spark.sql.functions.desc

    (flightData2015
    .groupBy("DEST_COUNTRY_NAME")
    .sum("count")
    .withColumnRenamed("sum(count)", "destination_total")
    .sort(desc("destination_total"))
    .limit(5)
    .show())
    ```
    - 실행계획은 트랜스포메이션의 지향성 비순환 그래프(Directed Acyclic Graph)이며, 액션이 호출되면 결과를 만들어낸다.
    - DAG의 각 단계는 불변성을 가진 신규 DataFrame이다.

    - DataFrame의 변환 흐름
        - csv -> read -> dataframe -> group by -> dataframe group -> sum -> dataframe -> 컬럼명 변경 -> dataframe -> sort -> dataframe -> limit -> dataframe -> collect -> Array(...)

## Dataset
- 타입 안정성을 제공해주는 `구조적 API`
- Java와 스칼라의 정적 데이터 타입에 맞는 코드
- `타입 안정성`을 지원하며 동적 타입 언어인 파이썬과 R에서는 사용할 수 없다.
- Dataset의 API는 DataFrame의 레코드를 사용자가 자바나 스칼라로 정의한 클래스에 할당하고 자바의 ArrayList 또는 스칼라의 Seq 객체 등의 고정 타입형 컬렉션으로 다룰 수 있는 기능을 제공한다.
- Dataset API는 초기화에 사용한 클래스 대신 다른 클래스를 사용해 접근할 수 없다. (타입 안정성을 보장해주기 때문에)
- Dataset 클래스는 내부 객체의 데이터타입을 매개변수로 사용한다.
    - Java: `Dataset<T>`
    - 스칼라: `Dataset[T]`
- collect 메소드나 take 메소드를 호출하면 DataFrame을 구성하는 Row 타입의 객체가 아닌 Dataset에 매개변수로 지정한 타입의 객체를 반환한다.
- 하여, 코드 변경 없이 타입 안정성을 보장할 수 있다.
- 하여, 로컬이나 분산 클러스터 환경에서 데이터를 안전하게 다룰 수 있다.

## 스파크의 기능
![image](https://user-images.githubusercontent.com/58713684/73913302-ff917200-48f9-11ea-8f04-b13c2a83ab2f.png)
- 히스토리: RDD -> DF -> DS

### 구조적 스트리밍
- 스파크 2.2버전에서 안정화된 고수준 API
- 구조적 스트리밍을 사용하면 구조적 API로 개발된 배치 모드의 연산을 스트리밍 방식으로 실행할 수 있으며, 지연 시간을 줄일 수 있다.
- 구조적 스트리밍은 배치처리용 코드를 일부 수정하여 스트리밍 처리를 수행하고 값을 빠르게 얻을 수 있다는 장점이 있다.
